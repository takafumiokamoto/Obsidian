# Self-hosted Runner on ECS with GitHub App and Docker-in-Docker
Created: 6/16/2025, 2:18:40 PM
Last Updated: 6/16/2025, 2:25:16 PM
---

### User

I want to set up a self-hosted runner for GitHub and have decided to use AWS ECS  for hosting.
However, I have two concerns with this approach:
Firstly, a Personal Access Token (PAT) is required to retrieve the runner registration token on startup, which I would like to avoid.
Secondly, since Fargate doesn't allow privileged containers, running Docker commands inside the self-hosted runner will be problematic. therefore, I want to use EC2 lauch type.
I would like you to create a comprehensive guide to solve this problem.
The guide must meet the following requirements:
- Use GitHub App for authentication.
- Use CloudFormation to create AWS resources.
- Use Python to create an entrypoint script for the Docker container.
- Use the docker-in-docker sidecar pattern.

To start, could you please show me the title of each step? I will ask you about them individually afterwards.


---

### Assistant (gemini-2.5-pro)

<details>
<summary>Reasoning</summary>

**Outlining the Guide Structure**

I've just begun deconstructing the user's request. My focus now is on understanding the core goal - setting up a self-hosted GitHub runner on AWS ECS. I'm noting the specific host, AWS ECS, and the EC2 launch type. I'm mapping the essential components. The goal is to provide a comprehensive guide.


**Developing the Step Sequence**

I'm now prioritizing the order of steps. My aim is to organize a logical flow, beginning with GitHub App setup, transitioning to AWS infrastructure provisioning via CloudFormation, and culminating in the Docker image creation and entrypoint script configuration. I'm focusing on a clean, linear presentation to guide the user seamlessly through the setup process.


**Refining Step Sequencing**

I'm now prioritizing the order of steps. My aim is to organize a logical flow, beginning with GitHub App setup, transitioning to AWS infrastructure provisioning via CloudFormation, and culminating in the Docker image creation and entrypoint script configuration. I'm focusing on a clean, linear presentation to guide the user seamlessly through the setup process.

**Adjusting Detail Granularity**

I'm currently focused on determining the level of detail for each step. The goal is a balance between helpful guidance and information overload. I'm striving for the right combination of high-level descriptions and actionable tips that provide sufficient context. I will aim to deliver a "comprehensive guide" to get the job done and tested.


**Finalizing Step Granularity**

I'm now focusing on the final step titles, ensuring they're clear and actionable. My goal is to craft titles that communicate purpose, not just action. I'm prioritizing clarity and context. I'll need to specify what a developer would need to do first, second, and so on. Now, I'm working to consolidate the sub-components within the CloudFormation step.


**Organizing Finalized Steps**

I've determined the final step titles, ensuring each conveys purpose. I've consolidated CloudFormation components into a single step, breaking it down further for clarity. My focus now is on deploying the ECS Service using the Task Definition. I'm also planning to create a container within the ECS to run the service. The goal is to provide a step-by-step guide.


**Defining Step Titles Iteratively**

I'm now iterating on the step titles, focusing on clarity and actionability. The goal is to ensure each title communicates both the 'what' and the 'why.' For example, I'm refining "Define AWS Infrastructure with CloudFormation" to incorporate the components it contains, such as networking, IAM roles, and ECS cluster configuration. This will give the user a clear expectation of what to expect within each step, thus, I can provide more information on sub-components in the CloudFormation step.


**Outlining Step Sequencing**

I'm now prioritizing the order of steps. My aim is to organize a logical flow, beginning with GitHub App setup, transitioning to AWS infrastructure provisioning via CloudFormation, and culminating in the Docker image creation and entrypoint script configuration. I'm focusing on a clean, linear presentation to guide the user seamlessly through the setup process.

I've revised the step sequencing to include authentication, infrastructure provisioning, container development, task definition creation with the DinD sidecar, ECS service creation, and deployment verification. This flow ensures a logical progression, starting from GitHub App setup and ending with a comprehensive solution. This also reflects a more user-centric approach that ensures all the essential elements are covered.



</details>

Of course. Creating a secure, scalable, and maintainable self-hosted runner environment on AWS ECS is an excellent project. Using a GitHub App is the modern, recommended approach for authentication, and the Docker-in-Docker (DinD) sidecar pattern is a clever way to handle Docker builds on the EC2 launch type.

Here is the proposed structure for the comprehensive guide, broken down into logical steps.

***

### **Guide: Self-Hosted GitHub Runners on AWS ECS with GitHub Apps and Docker-in-Docker**

**Step 1: Create and Configure a GitHub App for Authentication**

**Step 2: Generate a Private Key and Store Credentials in AWS Secrets Manager**

**Step 3: Define Core AWS Infrastructure with CloudFormation**

**Step 4: Develop the Runner Container Image with a Python Entrypoint**

**Step 5: Create the ECS Task Definition with a Docker-in-Docker (DinD) Sidecar**

**Step 6: Create the ECS Service to Run and Scale the Runners**

**Step 7: Deploy and Verify the Solution**

***

You can now ask me about each step individually, and I will provide the detailed explanation, code, and CloudFormation templates required.


---

### User

Let's jump to Step 5


---

### Assistant (gemini-2.5-pro)

<details>
<summary>Reasoning</summary>

**Elaborating the Task**

I'm now zeroing in on the core of the user's request. My current focus is to carefully dissect "Create the ECS Task Definition with a Docker-in-Docker (DinD) Sidecar." I'm mentally breaking down the components: the ECS Task Definition, the DinD sidecar, and the implications of this setup. The goal is a clear, concise explanation of Step 5.


**Refining the Approach**

I'm now focusing on a more structured approach to explain Step 5. I've broken down the request into key concepts: the Task Definition's purpose, the necessity of the DinD sidecar, and the crucial connections between the runner, the DinD container, and the secrets/permissions. I intend to clarify why this configuration is essential for running Docker within the ECS task.


**Outlining the Components**

I'm presently building the CloudFormation template, resource by resource. The focus is now on the `TaskDefinition`'s structure. I'm breaking down the template into manageable sections: parameters, the task role, container definitions, and volumes, ensuring a clear and organized explanation. My objective is to provide an easily digestible guide.


**Analyzing the Structure**

Now, I'm analyzing the CloudFormation template's structure in more detail. I'm focusing on the interaction between the runner and DinD containers within the `ContainerDefinitions`. The goal is to articulate the setup clearly: the volume mount, the shared Docker socket, and the runner's reliance on it. I also consider the security implications of this approach.

\n\n
**Revising the Template Logic**

I'm now revising my initial logic for the CloudFormation template, paying close attention to the `ContainerDefinitions` section. Specifically, the DinD container's configuration (image, resource limits, etc.) and the runner container's link to the DinD socket, and the volumes, which will be the volume definition for the `docker-socket`. I'm verifying the parameters and resources align with best practices and will achieve a robust and secure configuration.


**Configuring the Core Elements**

I'm now focusing on the environment variables and mount points within the `ContainerDefinitions`. It's crucial that the `DOCKER_HOST` points correctly to the shared Docker socket, enabling the runner to use the DinD container. I'm double-checking the volume mount configuration to ensure proper communication between the containers. I'm also ensuring the secrets retrieval and environmental variables are setup according to best practices.


**Clarifying the Communication**

I'm now focusing on how the runner and DinD containers communicate. The crucial element is the shared Docker socket. I will explain how the runner, using the Python script (from Step 4), interacts with the Docker daemon within the DinD container. This includes the `DOCKER_HOST` environment variable and volume mounting.


**Structuring the CloudFormation**

I'm now diving into the `ContainerDefinitions` section. My current focus is refining the runner and DinD configurations within the Task Definition.  I'm making sure `DOCKER_HOST` is correctly set, and the volume mounts and links are logical. Ensuring the GitHub App secrets are properly handled is a priority. I intend to provide a clear and secure approach.


**Orchestrating the Runners**

I'm now focusing on the central role of the `TaskDefinition` within the larger infrastructure. My goal is to craft a `ContainerDefinition` that ensures robust and secure communication. Specifically, I'm concentrating on the `docker-socket` volume, its mount, and the `DOCKER_HOST` environment variable to ensure the runner container can interact with the DinD container. This setup is crucial for executing Docker commands seamlessly.



</details>

Excellent. Let's dive into Step 5. This is where we define how our containers will run together to form a single, functional unit.

***

### **Step 5: Create the ECS Task Definition with a Docker-in-Docker (DinD) Sidecar**

A Task Definition is a blueprint for your application that tells ECS how to launch the containers. In our case, it will define two containers: our custom GitHub runner and a "Docker-in-Docker" (DinD) sidecar.

#### Why the Sidecar Pattern?

The core of this solution is the sidecar pattern.
1.  **GitHub Runner Container**: This container runs the official GitHub Actions runner agent. Its job is to communicate with GitHub, pick up jobs, and execute the steps defined in your workflow files. It will have the Docker CLI installed.
2.  **Docker-in-Docker (DinD) Sidecar**: This is a special container running the Docker daemon itself. Because it's configured as `privileged`, it can run a Docker service inside it, which is something the main runner container cannot do directly in a standard ECS setup.

#### How They Communicate

The two containers will share a common volume.
*   The DinD container will expose its Docker socket (`/var/run/docker.sock`) to this shared volume.
*   The GitHub runner container will mount this same volume to its own `/var/run/docker.sock`.

When the runner executes a `docker build` or `docker run` command, its local Docker CLI sends the command to `/var/run/docker.sock`. This command is seamlessly picked up by the Docker daemon running in the privileged DinD sidecar, which then executes the command.

#### CloudFormation Template: `task-definition.yml`

This CloudFormation template defines the `AWS::ECS::TaskDefinition` resource. It references the IAM roles we created in Step 3 and the runner image we built in Step 4.

Save the following code as `task-definition.yml`:

```yaml
AWSTemplateFormatVersion: "2010-09-09"
Description: "ECS Task Definition for GitHub self-hosted runner with DinD sidecar."

Parameters:
  AppName:
    Type: String
    Description: "The name of the application, used for naming resources."
    Default: "github-runner"
  RunnerImageUri:
    Type: String
    Description: "The full ECR URI of the GitHub runner container image."
  TaskCpu:
    Type: String
    Description: "CPU units to allocate for the task."
    Default: "1024" # 1 vCPU
  TaskMemory:
    Type: String
    Description: "Memory to allocate for the task (in MiB)."
    Default: "2048" # 2 GB
  ExecutionRoleArn:
    Type: String
    Description: "ARN of the ECS Task Execution Role."
  TaskRoleArn:
    Type: String
    Description: "ARN of the ECS Task Role for container permissions."
  SecretsManagerSecretArn:
    Type: String
    Description: "ARN of the AWS Secrets Manager secret for GitHub App credentials."

Resources:
  ECSTaskDefinition:
    Type: "AWS::ECS::TaskDefinition"
    Properties:
      Family: !Sub "${AppName}-task"
      NetworkMode: "awsvpc"
      RequiresCompatibilities:
        - "EC2"
      Cpu: !Ref TaskCpu
      Memory: !Ref TaskMemory
      ExecutionRoleArn: !Ref ExecutionRoleArn
      TaskRoleArn: !Ref TaskRoleArn
      Volumes:
        - Name: "docker-socket"
          # We don't define a host path here; Docker will manage it.
          # This creates a shared volume between the containers in the task.
      ContainerDefinitions:
        # Main GitHub Runner Container
        - Name: "github-runner"
          Image: !Ref RunnerImageUri
          Essential: true
          LogConfiguration:
            LogDriver: "awslogs"
            Options:
              awslogs-group: !Sub "/ecs/${AppName}"
              awslogs-region: !Ref AWS::Region
              awslogs-stream-prefix: "runner"
          Secrets:
            - Name: "GITHUB_APP_ID"
              ValueFrom: !Sub "${SecretsManagerSecretArn}:GITHUB_APP_ID::"
            - Name: "GITHUB_PRIVATE_KEY"
              ValueFrom: !Sub "${SecretsManagerSecretArn}:GITHUB_PRIVATE_KEY::"
            - Name: "GITHUB_INSTALLATION_ID"
              ValueFrom: !Sub "${SecretsManagerSecretArn}:GITHUB_INSTALLATION_ID::"
          Environment:
            - Name: "DOCKER_HOST"
              Value: "unix:///var/run/docker.sock"
            - Name: "RUNNER_SCOPE" # e.g., 'repo' or 'org'
              Value: "repo" # Change to 'org' if this is an organization runner
            - Name: "GITHUB_OWNER"
              Value: "your-github-username-or-org" # <-- TODO: Change this
            - Name: "GITHUB_REPOSITORY"
              Value: "your-github-repo" # <-- TODO: Change this
            - Name: "RUNNER_LABELS"
              Value: "ecs,ec2,docker"
          MountPoints:
            - SourceVolume: "docker-socket"
              ContainerPath: "/var/run/docker.sock"
          DependsOn:
            - ContainerName: "docker-in-docker"
              Condition: "START"

        # Docker-in-Docker (DinD) Sidecar Container
        - Name: "docker-in-docker"
          Image: "docker:25-dind" # Use a specific, stable version
          Essential: true
          Privileged: true # This is why we need the EC2 launch type
          LogConfiguration:
            LogDriver: "awslogs"
            Options:
              awslogs-group: !Sub "/ecs/${AppName}"
              awslogs-region: !Ref AWS::Region
              awslogs-stream-prefix: "dind"
          MountPoints:
            - SourceVolume: "docker-socket"
              ContainerPath: "/var/run/docker.sock"

Outputs:
  TaskDefinitionArn:
    Description: "The ARN of the created ECS Task Definition."
    Value: !Ref ECSTaskDefinition
```

#### Key Sections Explained

*   **`RequiresCompatibilities: ["EC2"]`**: This explicitly states that the task can only run on EC2 instances, not on Fargate. This is required for the `Privileged` flag.
*   **`Volumes`**: We define a single volume named `docker-socket`. This volume will be shared between our two containers.
*   **`ContainerDefinitions`**:
    *   **`github-runner`**:
        *   **`Secrets`**: This is how we securely pass the GitHub App credentials from AWS Secrets Manager into the container as environment variables. Our Python entrypoint script will use these.
        *   **`Environment`**:
            *   `DOCKER_HOST`: This is critical. It tells the Docker CLI inside this container to communicate with the daemon via the shared socket file instead of its default path.
            *   `GITHUB_OWNER` / `GITHUB_REPOSITORY`: **You must update these** to point to your repository. The Python script uses these to request the correct registration token.
        *   **`MountPoints`**: Mounts the shared `docker-socket` volume to the standard Docker socket path.
        *   **`DependsOn`**: Ensures the `docker-in-docker` container is started before this one attempts to.
    *   **`docker-in-docker`**:
        *   **`Image: "docker:25-dind"`**: We use the official Docker image with the `-dind` (Docker-in-Docker) tag.
        *   **`Privileged: true`**: This gives the container the elevated permissions needed to run a Docker daemon.
        *   **`MountPoints`**: It also mounts the `docker-socket` volume, making its daemon available to other containers.

With this Task Definition, you have a complete, reusable blueprint for a runner that can securely authenticate with GitHub and build Docker images.

***

Next, we will use this Task Definition to create an ECS Service that manages the lifecycle of our runners. What would you like to know about **Step 6: Create the ECS Service to Run and Scale the Runners**?


---

### User

since We are setting up ECS, Can we use 'DOCKER_HOST' instead of mouting socket?


---

### Assistant (gemini-2.5-pro)

<details>
<summary>Reasoning</summary>

**Exploring the Docker Connection**

I'm currently focused on the user's core inquiry: whether `DOCKER_HOST` can fully replace socket mounting. My analysis now zeroes in on the practical implications of each method. I'm building a comparison to highlight the nuances and potential trade-offs. I'm diving deeper into how `DOCKER_HOST` configuration impacts Docker client-daemon communication vs. direct socket access.


**Defining Roles of each method**

Now, I'm focusing on the distinct functions of `DOCKER_HOST` and socket mounting, emphasizing their co-dependence. My thinking is progressing; I will explain why they are used together. I will use analogies to help the user understand the concepts. The analogy will compare it to a physical mail room: `DOCKER_HOST` would be the address, the shared volume will be the mailbox with the connection key. I'm aiming for clarity in explaining the 'why' behind using them together, avoiding jargon.


**Refining the Analogy**

I am now focusing on refining the analogy to clearly illustrate the combined functions of `DOCKER_HOST` and socket mounting. I will incorporate the analogy in explaining the server, the connection, and the client. I'm also ensuring the user understands why setting the `DOCKER_HOST` to the same default path is good practice, but not the only step.


**Connecting the Dots**

My thoughts now center on the necessity of both the environment variable and volume mounting for Docker client-daemon interaction. I'm building out a comparison table, illustrating the individual functions, and explaining how they work in harmony. I'm focusing on why setting `DOCKER_HOST` alone doesn't suffice without establishing a physical link between the containers. I'm thinking about the user's intent to connect two daemons, and highlighting the importance of clear communication pathways.


**Analyzing the Core Question**

I'm now fully immersed in understanding the user's core inquiry: whether the `DOCKER_HOST` alone can replace the socket mount. I see that their question comes from deeper thinking about the mechanics of container communication. I'm focusing on why a shared path is essential, drawing from the "address vs. road" analogy.


**Understanding the Connection Path**

I'm now fully immersed in understanding the user's core inquiry: whether the `DOCKER_HOST` alone can replace the socket mount. I see that their question comes from deeper thinking about the mechanics of container communication. I'm focusing on why a shared path is essential, drawing from the "address vs. road" analogy.



</details>

That's an excellent and insightful question. It gets to the very heart of how these two containers communicate.

The short answer is: **No, you cannot use `DOCKER_HOST` *instead* of mounting the socket. You must use them together.**

Let's break down why. Think of it like a physical address and the actual road leading to it.

1.  **The Shared Volume Mount (`-v /var/run/docker.sock:/var/run/docker.sock`)**: This is the **physical connection**. It's the road. The DinD container *creates* the `docker.sock` file, which is a Unix socket that acts as the API endpoint for the Docker daemon. By mounting this socket into the runner container, you are making that physical endpoint accessible inside the runner. Without this mount, the runner container has no "road" to the Docker daemon; the `docker.sock` file simply doesn't exist from its perspective.

2.  **The Environment Variable (`DOCKER_HOST=unix:///var/run/docker.sock`)**: This is the **address**. It tells the Docker CLI *where to look* for the Docker daemon's API endpoint.

Hereâ€™s a step-by-step of what happens:

1.  The **DinD sidecar** starts up. Because it's a privileged container running the Docker daemon, it creates the file `/var/run/docker.sock` inside its own filesystem.
2.  The **shared volume** that we defined in the Task Definition (`docker-socket`) acts as a bridge. It takes that `/var/run/docker.sock` file from the DinD container and makes it visible at `/var/run/docker.sock` inside the **runner container**.
3.  The **runner container** starts. Your workflow tells it to run a command like `docker build .`.
4.  The Docker CLI inside the runner container checks the `DOCKER_HOST` environment variable to know where to send this command.
5.  It sees `DOCKER_HOST=unix:///var/run/docker.sock`, so it tries to communicate with the file at that path.
6.  Because of the volume mount, that file is the *exact same file* being served by the DinD container's daemon. The command is successfully passed to the DinD container, which executes the build.

#### What Happens If You Omit One?

*   **No Volume Mount, `DOCKER_HOST` set**: The Docker CLI in the runner looks for `/var/run/docker.sock`, but the file doesn't exist in its filesystem. You will get an error like: `Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?`
*   **Volume Mount, `DOCKER_HOST` not set**: The Docker CLI will use its default path, which is also `unix:///var/run/docker.sock`. So, in many cases, this would *actually work*. However, explicitly setting `DOCKER_HOST` is a critical best practice for clarity, debugging, and ensuring your setup is robust and not reliant on default behavior.

### The TCP Alternative (and why we still prefer the socket)

Your question might also be inspired by another way to use `DOCKER_HOST`, which is over TCP. In that scenario:

*   The DinD container would be configured to expose the Docker daemon on a TCP port, for example, `2375`.
*   The runner container would set `DOCKER_HOST=tcp://localhost:2375`.
*   In this case, you would **not** need the volume mount, because the communication happens over the task's internal network (`localhost`) instead of a shared file.

However, for the ECS sidecar pattern, the **shared socket is the standard and recommended approach** for two main reasons:

1.  **Security**: Exposing the Docker daemon over a TCP port, even on localhost, is generally considered less secure than using a Unix socket, which can be protected by standard filesystem permissions.
2.  **Performance**: Communication via a Unix socket is more efficient and has lower overhead than a TCP network connection, even on the loopback interface.

### Summary

| Component | Analogy | Purpose |
| :--- | :--- | :--- |
| **DinD Container** | Power Plant | Creates and runs the Docker daemon service. |
| **Shared Volume** | Power Line | Physically connects the daemon (power plant) to the runner (house). |
| **`DOCKER_HOST` Variable** | Wall Outlet Label | Tells the Docker CLI (appliance) which outlet to use. |

You need all three components for the system to work correctly. The CloudFormation template in our guide correctly configures both the shared volume and the environment variable to establish this robust connection.


---
